{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EC414_HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8FRqkmo6pk"
      },
      "source": [
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_ih-R1fJ6VG"
      },
      "source": [
        "# Homework 4: Linear Binary Classification Methods\n",
        "by Junyu Liu and Brian Kulis\n",
        "\n",
        "**Due date**: March 3, Wednesday by 11:59pm\n",
        "\n",
        "**Late** due date: March 6, Saturday by 11:59pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxGJCzSKovR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mxor3LzKpqj"
      },
      "source": [
        "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\n",
        "\n",
        "To use Google Colab:\n",
        "\n",
        "1. Download this `ipynb` file.\n",
        "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\n",
        "3. Upload this file. It will then open in Colab.\n",
        "\n",
        "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\n",
        "\n",
        "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\n",
        "\n",
        "**Submission instructions:** please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by the due date (see Schedule)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTtN4g49nWrB"
      },
      "source": [
        "This homework is scored out of 100.\n",
        "\n",
        "\n",
        "**Important:** unless otherwise specified, you should **NOT** use loops. This is not to say loops are always bad, but avoiding them should help you:\n",
        "1.   get more familiar with common language features and libraries\n",
        "2.   write more efficient code\n",
        "3.   \"think in higher dimensions\" (get more comfortable with vectors, matrices, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32VwlhsN9ny"
      },
      "source": [
        "## **Question 1:** Creating a Dataset (10 points)\n",
        "\n",
        "For this assignment, we will create a simple linearly-separable dataset for binary classification. We have provided you with the code to generate the feature vectors. Notice that one class has significantly more samples than the other.\n",
        "\n",
        "**Important:** Although this dataset has only 1 feature, **ALL** the code you write in this assignment should be able to run as intended with more features. The only exception is where you are producing plots. Many functions you need to write will be tested for compatibility with more features in question 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS0rlO3eOCZK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import default_rng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw5AUAsV5MQr"
      },
      "source": [
        "# Do NOT change\n",
        "rng = default_rng(1)\n",
        "x1 = rng.uniform(-4, 2, 460)\n",
        "x2 = rng.normal(5, np.sqrt(3), 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7rbmFam-xWg"
      },
      "source": [
        "### Problem a. (4 points)\n",
        "We need to create appropriate labels for the two classes. x1 is the feature vectors of the negative class and x2 the positive class. Also produce a colored feature-label scatter plot (the negative class should be blue and the positive class should be red)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "1i19iWdkAVpY",
        "outputId": "afd1cc66-1ec6-41fc-f4eb-751f2a3e6dc3"
      },
      "source": [
        "# WRITE CODE HERE: \n",
        "s1 = np.size(x1)\n",
        "s2 = np.size(x2)\n",
        "y1 = np.ones(s1) * -1\n",
        "y2 = np.ones(s2) *1 \n",
        "plt.scatter(x1, y1, color = 'blue')\n",
        "plt.scatter(x2,y2,color='red')\n",
        "plt.title('Scatter Plot')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Scatter Plot')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRLTGhczArfW"
      },
      "source": [
        "### Problem b. (3 points)\n",
        "Combine the data into X and y, and then perform a 75-25 train-test split. Use 32 as the random_state.\n",
        "\n",
        "Hint: X has to be a 2D array, so you need to use reshape at some point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofFs0MR9Dp7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaf2be6-7306-4498-d69d-49ee68587239"
      },
      "source": [
        "# WRITE CODE HERE:\n",
        "X = np.concatenate((x1,x2),axis=0)\n",
        "y = np.concatenate((y1,y2),axis=0)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "X = X.reshape((500,1))\n",
        "y = y.reshape((500,1))\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=32, shuffle=True)# -- Code Required --\n",
        "print(\"Xtrain\")\n",
        "print(xtrain)\n",
        "print(\"Shape of xtrain\")\n",
        "print(xtrain.shape)\n",
        "print(\"Xtest\")\n",
        "print(xtest)\n",
        "print(\"Shape of xtest\")\n",
        "print(xtest.shape)\n",
        "print(\"ytrain\")\n",
        "print(ytrain)\n",
        "print(\"Shape of ytrain\")\n",
        "print(ytrain.shape)\n",
        "print(\"ytest\")\n",
        "print(ytest)\n",
        "print(\"Shape of ytest\")\n",
        "print(ytest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500,)\n",
            "(500,)\n",
            "(500, 1)\n",
            "(500, 1)\n",
            "Xtrain\n",
            "[[-2.54070721]\n",
            " [-2.67418502]\n",
            " [-3.10571186]\n",
            " [-0.5398017 ]\n",
            " [-0.26611982]\n",
            " [-2.30097366]\n",
            " [-0.32198019]\n",
            " [-1.16351213]\n",
            " [-2.17789568]\n",
            " [-1.47286711]\n",
            " [-0.6250596 ]\n",
            " [-0.90906818]\n",
            " [ 1.7437662 ]\n",
            " [-3.05271398]\n",
            " [-0.94302471]\n",
            " [ 3.64484591]\n",
            " [-2.81563449]\n",
            " [ 0.39416502]\n",
            " [-1.0092425 ]\n",
            " [-1.87989533]\n",
            " [-3.30480633]\n",
            " [-2.36928921]\n",
            " [ 4.84175885]\n",
            " [ 1.68203701]\n",
            " [-2.06749188]\n",
            " [ 1.20392303]\n",
            " [-1.2439847 ]\n",
            " [ 0.95292688]\n",
            " [ 1.54389875]\n",
            " [-1.97287478]\n",
            " [-3.20692695]\n",
            " [-0.93466669]\n",
            " [-0.16316838]\n",
            " [-0.74958184]\n",
            " [ 1.24664721]\n",
            " [ 1.50489064]\n",
            " [-1.27901266]\n",
            " [-3.52034497]\n",
            " [ 0.16173701]\n",
            " [ 1.70278218]\n",
            " [-3.53749715]\n",
            " [-0.69809049]\n",
            " [ 0.15717339]\n",
            " [ 4.68088349]\n",
            " [ 0.87649786]\n",
            " [ 1.97684714]\n",
            " [-2.33719999]\n",
            " [-0.54576904]\n",
            " [-2.8492077 ]\n",
            " [-2.77926856]\n",
            " [-1.41263951]\n",
            " [ 1.8844232 ]\n",
            " [ 1.56083121]\n",
            " [-2.6649588 ]\n",
            " [-0.10689548]\n",
            " [-0.75315368]\n",
            " [-0.20948685]\n",
            " [-0.27807667]\n",
            " [-3.32095504]\n",
            " [ 0.8746826 ]\n",
            " [-3.85305594]\n",
            " [-1.4874819 ]\n",
            " [ 1.69189668]\n",
            " [-3.02627599]\n",
            " [ 5.68402393]\n",
            " [-1.67338531]\n",
            " [-3.87121674]\n",
            " [-1.7422162 ]\n",
            " [-0.20718929]\n",
            " [-2.45318129]\n",
            " [ 0.94689286]\n",
            " [-2.18083102]\n",
            " [ 0.67148221]\n",
            " [-1.80995642]\n",
            " [-3.98765894]\n",
            " [-3.5106843 ]\n",
            " [ 1.63251097]\n",
            " [-0.03786772]\n",
            " [-3.03608795]\n",
            " [-0.82466315]\n",
            " [-2.70127852]\n",
            " [-3.22796073]\n",
            " [ 5.13649924]\n",
            " [-0.74011953]\n",
            " [-1.12579891]\n",
            " [ 1.81955248]\n",
            " [ 0.05406853]\n",
            " [-0.31732865]\n",
            " [ 1.36829517]\n",
            " [-0.9381805 ]\n",
            " [ 1.03810762]\n",
            " [ 9.42168222]\n",
            " [-3.58954219]\n",
            " [-3.41370605]\n",
            " [ 0.35854784]\n",
            " [-1.84127844]\n",
            " [ 0.08863072]\n",
            " [-0.84622286]\n",
            " [-0.87084927]\n",
            " [ 1.87703747]\n",
            " [-1.58132208]\n",
            " [-0.75993277]\n",
            " [-2.39685049]\n",
            " [-0.70243787]\n",
            " [ 0.98750665]\n",
            " [ 0.74680369]\n",
            " [-3.29974086]\n",
            " [ 1.90249958]\n",
            " [ 1.95011718]\n",
            " [-2.12489791]\n",
            " [ 1.68144   ]\n",
            " [-3.43847888]\n",
            " [-3.56085957]\n",
            " [-3.13504232]\n",
            " [-2.49988996]\n",
            " [-0.20600208]\n",
            " [ 5.88494294]\n",
            " [ 2.36320375]\n",
            " [-1.19986118]\n",
            " [ 0.50218804]\n",
            " [ 1.92931341]\n",
            " [-1.47447163]\n",
            " [-1.83841669]\n",
            " [-3.7966465 ]\n",
            " [ 5.7520238 ]\n",
            " [-0.77114012]\n",
            " [-0.65554672]\n",
            " [-1.46369856]\n",
            " [ 1.02482853]\n",
            " [-0.30169725]\n",
            " [-3.89903629]\n",
            " [-1.65955269]\n",
            " [-3.85666712]\n",
            " [ 1.16760011]\n",
            " [-2.81953827]\n",
            " [-2.5266864 ]\n",
            " [-0.86667983]\n",
            " [ 1.08199022]\n",
            " [ 0.11122154]\n",
            " [ 0.34873964]\n",
            " [ 0.48930775]\n",
            " [ 1.68293474]\n",
            " [-2.12171285]\n",
            " [-1.74256899]\n",
            " [-3.79572651]\n",
            " [-2.28720863]\n",
            " [-1.06930464]\n",
            " [-1.22055978]\n",
            " [ 0.555611  ]\n",
            " [ 0.98764901]\n",
            " [-3.0129564 ]\n",
            " [-2.60860073]\n",
            " [-0.67765212]\n",
            " [ 1.30712919]\n",
            " [ 7.1940367 ]\n",
            " [-3.01755007]\n",
            " [-3.54873333]\n",
            " [ 6.23830444]\n",
            " [-1.25152264]\n",
            " [-2.7927808 ]\n",
            " [-0.40285316]\n",
            " [ 1.13547377]\n",
            " [-1.08877623]\n",
            " [-3.96505243]\n",
            " [-3.6494653 ]\n",
            " [ 0.24952877]\n",
            " [-2.31754745]\n",
            " [ 6.02181143]\n",
            " [-1.34326666]\n",
            " [ 0.40154277]\n",
            " [ 0.02183988]\n",
            " [ 1.76994316]\n",
            " [-2.97876295]\n",
            " [-1.62246303]\n",
            " [-0.49020251]\n",
            " [-3.13598275]\n",
            " [ 0.53132946]\n",
            " [ 0.64336579]\n",
            " [-0.75263887]\n",
            " [-3.98351711]\n",
            " [-1.96483064]\n",
            " [ 1.83013985]\n",
            " [ 1.64560513]\n",
            " [-2.72101194]\n",
            " [ 0.80002381]\n",
            " [-0.90358849]\n",
            " [-0.55814001]\n",
            " [-1.89385682]\n",
            " [-0.63702134]\n",
            " [ 0.99856861]\n",
            " [-3.00050094]\n",
            " [ 1.78980646]\n",
            " [-2.14619081]\n",
            " [-2.34212192]\n",
            " [-0.6607631 ]\n",
            " [-3.44204804]\n",
            " [ 0.35176363]\n",
            " [-1.36038112]\n",
            " [-2.27430639]\n",
            " [-3.60662109]\n",
            " [-1.90369385]\n",
            " [-2.4986816 ]\n",
            " [ 1.16770098]\n",
            " [ 1.49184961]\n",
            " [ 0.86164611]\n",
            " [ 4.6557008 ]\n",
            " [-1.56406737]\n",
            " [-1.29376668]\n",
            " [ 0.64798481]\n",
            " [-1.76887258]\n",
            " [-2.43781467]\n",
            " [-0.9771823 ]\n",
            " [ 0.74993723]\n",
            " [-2.27003207]\n",
            " [ 5.28397811]\n",
            " [-0.16401172]\n",
            " [-3.77824181]\n",
            " [ 0.18736142]\n",
            " [-0.01359375]\n",
            " [-2.87923792]\n",
            " [-2.10004935]\n",
            " [ 1.43001542]\n",
            " [-1.60425392]\n",
            " [-0.12567181]\n",
            " [ 1.11579703]\n",
            " [-1.50148031]\n",
            " [ 0.72076857]\n",
            " [-2.69938311]\n",
            " [ 0.44566672]\n",
            " [ 0.81418497]\n",
            " [-1.45321849]\n",
            " [-3.60820774]\n",
            " [-1.16854168]\n",
            " [-3.76244274]\n",
            " [-0.21809347]\n",
            " [-1.00608129]\n",
            " [-3.50885202]\n",
            " [-3.11246779]\n",
            " [-3.20382221]\n",
            " [ 0.96621556]\n",
            " [-0.20104032]\n",
            " [-3.0738782 ]\n",
            " [-3.68694245]\n",
            " [ 1.78202524]\n",
            " [ 1.77042049]\n",
            " [-2.72995154]\n",
            " [ 6.92308208]\n",
            " [-3.89966307]\n",
            " [ 0.91776031]\n",
            " [ 1.92463606]\n",
            " [ 0.83621425]\n",
            " [-3.0486698 ]\n",
            " [ 1.84480658]\n",
            " [ 6.73146265]\n",
            " [-0.41807362]\n",
            " [-3.84109271]\n",
            " [ 5.28107201]\n",
            " [-3.19519615]\n",
            " [ 1.49321842]\n",
            " [ 0.35884166]\n",
            " [-1.36340404]\n",
            " [-3.72080079]\n",
            " [-0.92907025]\n",
            " [-1.79380258]\n",
            " [ 5.24977784]\n",
            " [ 1.47609974]\n",
            " [-1.26442622]\n",
            " [-1.65157558]\n",
            " [ 0.83809361]\n",
            " [-3.62891839]\n",
            " [-0.62769046]\n",
            " [-3.72407692]\n",
            " [ 3.57883   ]\n",
            " [-2.1500863 ]\n",
            " [-0.73798426]\n",
            " [-3.2104387 ]\n",
            " [-0.15993665]\n",
            " [-1.54480518]\n",
            " [-2.43565167]\n",
            " [ 0.04075932]\n",
            " [ 0.61263056]\n",
            " [-2.3606993 ]\n",
            " [-2.11519663]\n",
            " [-3.68995637]\n",
            " [-3.66476821]\n",
            " [ 0.96095198]\n",
            " [-3.34159202]\n",
            " [ 0.0295258 ]\n",
            " [-3.88099513]\n",
            " [ 1.04736796]\n",
            " [ 0.72258165]\n",
            " [-0.36590111]\n",
            " [ 1.66937546]\n",
            " [-2.72301403]\n",
            " [-3.23427588]\n",
            " [-3.44746621]\n",
            " [-0.38905931]\n",
            " [-0.12257862]\n",
            " [ 0.47569028]\n",
            " [-2.95369658]\n",
            " [-0.36095146]\n",
            " [-2.42711922]\n",
            " [-3.23911151]\n",
            " [-2.9330453 ]\n",
            " [ 0.830329  ]\n",
            " [ 0.77927666]\n",
            " [-1.94923166]\n",
            " [-1.71745438]\n",
            " [ 1.82959048]\n",
            " [-0.36918046]\n",
            " [-3.00276901]\n",
            " [-1.47756457]\n",
            " [ 5.33881459]\n",
            " [-1.34842353]\n",
            " [-2.22859447]\n",
            " [-1.71411076]\n",
            " [ 0.09972144]\n",
            " [-3.95744903]\n",
            " [-3.70309729]\n",
            " [-0.46298763]\n",
            " [-2.95098677]\n",
            " [ 1.79997437]\n",
            " [ 1.77718747]\n",
            " [ 0.69656281]\n",
            " [-3.44971158]\n",
            " [ 8.17987483]\n",
            " [-2.33290953]\n",
            " [-2.4587952 ]\n",
            " [-1.14030048]\n",
            " [ 5.49511218]\n",
            " [ 1.0134153 ]\n",
            " [-3.86429363]\n",
            " [ 0.14802221]\n",
            " [ 1.71217561]\n",
            " [-2.85030245]\n",
            " [-1.64504934]\n",
            " [-0.48089004]\n",
            " [-2.20110675]\n",
            " [-0.12567463]\n",
            " [ 1.7790903 ]\n",
            " [ 1.51453172]\n",
            " [-1.62666147]\n",
            " [-0.09596542]\n",
            " [ 0.54323796]\n",
            " [-2.17894644]\n",
            " [ 5.45930528]\n",
            " [-0.07680393]\n",
            " [-0.14229932]\n",
            " [ 7.78478936]\n",
            " [ 0.13778093]\n",
            " [ 0.61110199]\n",
            " [-0.44235389]\n",
            " [-1.20440055]\n",
            " [ 1.50378623]\n",
            " [ 1.20822857]\n",
            " [-2.30873304]\n",
            " [ 6.735317  ]\n",
            " [ 7.35101315]\n",
            " [-0.99786142]\n",
            " [-2.12901129]\n",
            " [-3.83464532]\n",
            " [ 1.72490802]\n",
            " [ 3.84801155]\n",
            " [-1.51209903]\n",
            " [-2.099571  ]\n",
            " [-1.10672567]\n",
            " [-1.42321513]\n",
            " [-2.32739214]\n",
            " [-1.74911802]\n",
            " [-1.46090803]\n",
            " [-1.85523552]\n",
            " [-1.68647646]\n",
            " [-3.39716626]\n",
            " [-2.43941531]\n",
            " [-0.33052162]]\n",
            "Shape of xtrain\n",
            "(375, 1)\n",
            "Xtest\n",
            "[[-2.33865278]\n",
            " [-1.80493882]\n",
            " [-1.82381886]\n",
            " [-0.54185159]\n",
            " [-0.15091846]\n",
            " [-2.92856873]\n",
            " [-3.62369246]\n",
            " [ 8.06138089]\n",
            " [-0.66411377]\n",
            " [ 4.76830383]\n",
            " [ 6.08010598]\n",
            " [ 5.09012288]\n",
            " [-2.42611996]\n",
            " [ 0.71051176]\n",
            " [ 1.555785  ]\n",
            " [ 0.88338695]\n",
            " [ 1.21920436]\n",
            " [-2.85205644]\n",
            " [ 1.50615354]\n",
            " [-2.81590681]\n",
            " [ 0.95734008]\n",
            " [-3.17306334]\n",
            " [-1.68605293]\n",
            " [-0.31663003]\n",
            " [-1.69590749]\n",
            " [-0.80004295]\n",
            " [-1.46004131]\n",
            " [-1.67961656]\n",
            " [ 1.25922258]\n",
            " [-3.80751191]\n",
            " [-2.708691  ]\n",
            " [-3.2648461 ]\n",
            " [-3.60760621]\n",
            " [-2.87305154]\n",
            " [ 0.3194563 ]\n",
            " [ 1.79496872]\n",
            " [-3.29136864]\n",
            " [-2.35570967]\n",
            " [ 1.78676804]\n",
            " [-1.68490604]\n",
            " [-2.77655572]\n",
            " [ 1.03928913]\n",
            " [-0.82846442]\n",
            " [ 4.27680766]\n",
            " [-2.78055074]\n",
            " [-1.43409369]\n",
            " [-3.19574982]\n",
            " [-2.34287727]\n",
            " [ 1.07013855]\n",
            " [-0.91919306]\n",
            " [-3.76875166]\n",
            " [-3.62590253]\n",
            " [-1.03857064]\n",
            " [-1.59018266]\n",
            " [ 0.83269171]\n",
            " [-1.59313503]\n",
            " [-1.71317481]\n",
            " [-1.08885415]\n",
            " [-0.23831201]\n",
            " [ 0.14777241]\n",
            " [ 0.5787712 ]\n",
            " [-0.77816752]\n",
            " [-2.73160931]\n",
            " [-2.28105385]\n",
            " [ 5.45171106]\n",
            " [ 0.52107865]\n",
            " [-3.37550179]\n",
            " [ 4.98374009]\n",
            " [-1.31912229]\n",
            " [ 5.47054347]\n",
            " [ 0.04813637]\n",
            " [-0.15203099]\n",
            " [ 1.84815688]\n",
            " [-1.63764872]\n",
            " [ 0.73057222]\n",
            " [ 0.66009869]\n",
            " [-0.26143726]\n",
            " [-0.01009452]\n",
            " [-1.13698314]\n",
            " [ 1.3131216 ]\n",
            " [-2.0216097 ]\n",
            " [-2.42503172]\n",
            " [-1.83288792]\n",
            " [ 0.99821047]\n",
            " [ 1.09426713]\n",
            " [-3.253238  ]\n",
            " [-0.57661241]\n",
            " [-3.05648867]\n",
            " [ 0.56473271]\n",
            " [-3.36447258]\n",
            " [ 0.30423036]\n",
            " [ 0.52019251]\n",
            " [-3.50772996]\n",
            " [ 4.597577  ]\n",
            " [-1.7110424 ]\n",
            " [-3.09685102]\n",
            " [ 5.03962554]\n",
            " [ 0.96557098]\n",
            " [ 0.54983517]\n",
            " [-2.74431374]\n",
            " [-3.0619201 ]\n",
            " [ 4.58595523]\n",
            " [-3.17686027]\n",
            " [-0.25906147]\n",
            " [-3.0362582 ]\n",
            " [-2.49239453]\n",
            " [-2.22996134]\n",
            " [ 3.46733963]\n",
            " [ 1.13136185]\n",
            " [-2.82221869]\n",
            " [ 0.26821426]\n",
            " [-2.74002397]\n",
            " [ 0.51818125]\n",
            " [-2.41396163]\n",
            " [ 0.29632397]\n",
            " [-0.96361046]\n",
            " [ 0.84453669]\n",
            " [-0.24843109]\n",
            " [-1.03596547]\n",
            " [-2.53904373]\n",
            " [-1.13755597]\n",
            " [ 1.99415529]\n",
            " [ 2.95167164]\n",
            " [-1.30962214]\n",
            " [-2.29716107]]\n",
            "Shape of xtest\n",
            "(125, 1)\n",
            "ytrain\n",
            "[[-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]]\n",
            "Shape of ytrain\n",
            "(375, 1)\n",
            "ytest\n",
            "[[-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " [ 1.]\n",
            " [-1.]\n",
            " [-1.]]\n",
            "Shape of ytest\n",
            "(125, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlkYTs619A_"
      },
      "source": [
        "### Problem c. (3 points)\n",
        "To conveniently account for the bias term in later parts, we will also store the feature vectors in extended form (each feature vector appended with 1). Apply this to both X_train and X_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LFCgyVU3EKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fd54e6-6289-485f-b853-04406b32bdca"
      },
      "source": [
        "#WRITE CODE HERE:\n",
        "ones_tr = np.ones((375, 1))\n",
        "ones_te = np.ones((125,1))\n",
        "\n",
        "# Xtr_ext = np.concatenate((xtrain, ones_tr), axis=1)\n",
        "# Xte_ext = np.concatenate((xtest, ones_te), axis=1)\n",
        "Xtr_ext = np.c_[xtrain, np.ones(xtrain.shape[0])]\n",
        "Xte_ext = np.c_[xtest, np.ones(xtest.shape[0])]\n",
        "print(\"xtrain_ext:\")\n",
        "print(Xtr_ext)\n",
        "print(\"shape of xtrain_ext\")\n",
        "print(Xtr_ext.shape)\n",
        "\n",
        "print(\"xtest_ext:\")\n",
        "print(Xte_ext)\n",
        "print(\"shape of xtest_ext\")\n",
        "print(Xte_ext.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xtrain_ext:\n",
            "[[-2.54070721  1.        ]\n",
            " [-2.67418502  1.        ]\n",
            " [-3.10571186  1.        ]\n",
            " [-0.5398017   1.        ]\n",
            " [-0.26611982  1.        ]\n",
            " [-2.30097366  1.        ]\n",
            " [-0.32198019  1.        ]\n",
            " [-1.16351213  1.        ]\n",
            " [-2.17789568  1.        ]\n",
            " [-1.47286711  1.        ]\n",
            " [-0.6250596   1.        ]\n",
            " [-0.90906818  1.        ]\n",
            " [ 1.7437662   1.        ]\n",
            " [-3.05271398  1.        ]\n",
            " [-0.94302471  1.        ]\n",
            " [ 3.64484591  1.        ]\n",
            " [-2.81563449  1.        ]\n",
            " [ 0.39416502  1.        ]\n",
            " [-1.0092425   1.        ]\n",
            " [-1.87989533  1.        ]\n",
            " [-3.30480633  1.        ]\n",
            " [-2.36928921  1.        ]\n",
            " [ 4.84175885  1.        ]\n",
            " [ 1.68203701  1.        ]\n",
            " [-2.06749188  1.        ]\n",
            " [ 1.20392303  1.        ]\n",
            " [-1.2439847   1.        ]\n",
            " [ 0.95292688  1.        ]\n",
            " [ 1.54389875  1.        ]\n",
            " [-1.97287478  1.        ]\n",
            " [-3.20692695  1.        ]\n",
            " [-0.93466669  1.        ]\n",
            " [-0.16316838  1.        ]\n",
            " [-0.74958184  1.        ]\n",
            " [ 1.24664721  1.        ]\n",
            " [ 1.50489064  1.        ]\n",
            " [-1.27901266  1.        ]\n",
            " [-3.52034497  1.        ]\n",
            " [ 0.16173701  1.        ]\n",
            " [ 1.70278218  1.        ]\n",
            " [-3.53749715  1.        ]\n",
            " [-0.69809049  1.        ]\n",
            " [ 0.15717339  1.        ]\n",
            " [ 4.68088349  1.        ]\n",
            " [ 0.87649786  1.        ]\n",
            " [ 1.97684714  1.        ]\n",
            " [-2.33719999  1.        ]\n",
            " [-0.54576904  1.        ]\n",
            " [-2.8492077   1.        ]\n",
            " [-2.77926856  1.        ]\n",
            " [-1.41263951  1.        ]\n",
            " [ 1.8844232   1.        ]\n",
            " [ 1.56083121  1.        ]\n",
            " [-2.6649588   1.        ]\n",
            " [-0.10689548  1.        ]\n",
            " [-0.75315368  1.        ]\n",
            " [-0.20948685  1.        ]\n",
            " [-0.27807667  1.        ]\n",
            " [-3.32095504  1.        ]\n",
            " [ 0.8746826   1.        ]\n",
            " [-3.85305594  1.        ]\n",
            " [-1.4874819   1.        ]\n",
            " [ 1.69189668  1.        ]\n",
            " [-3.02627599  1.        ]\n",
            " [ 5.68402393  1.        ]\n",
            " [-1.67338531  1.        ]\n",
            " [-3.87121674  1.        ]\n",
            " [-1.7422162   1.        ]\n",
            " [-0.20718929  1.        ]\n",
            " [-2.45318129  1.        ]\n",
            " [ 0.94689286  1.        ]\n",
            " [-2.18083102  1.        ]\n",
            " [ 0.67148221  1.        ]\n",
            " [-1.80995642  1.        ]\n",
            " [-3.98765894  1.        ]\n",
            " [-3.5106843   1.        ]\n",
            " [ 1.63251097  1.        ]\n",
            " [-0.03786772  1.        ]\n",
            " [-3.03608795  1.        ]\n",
            " [-0.82466315  1.        ]\n",
            " [-2.70127852  1.        ]\n",
            " [-3.22796073  1.        ]\n",
            " [ 5.13649924  1.        ]\n",
            " [-0.74011953  1.        ]\n",
            " [-1.12579891  1.        ]\n",
            " [ 1.81955248  1.        ]\n",
            " [ 0.05406853  1.        ]\n",
            " [-0.31732865  1.        ]\n",
            " [ 1.36829517  1.        ]\n",
            " [-0.9381805   1.        ]\n",
            " [ 1.03810762  1.        ]\n",
            " [ 9.42168222  1.        ]\n",
            " [-3.58954219  1.        ]\n",
            " [-3.41370605  1.        ]\n",
            " [ 0.35854784  1.        ]\n",
            " [-1.84127844  1.        ]\n",
            " [ 0.08863072  1.        ]\n",
            " [-0.84622286  1.        ]\n",
            " [-0.87084927  1.        ]\n",
            " [ 1.87703747  1.        ]\n",
            " [-1.58132208  1.        ]\n",
            " [-0.75993277  1.        ]\n",
            " [-2.39685049  1.        ]\n",
            " [-0.70243787  1.        ]\n",
            " [ 0.98750665  1.        ]\n",
            " [ 0.74680369  1.        ]\n",
            " [-3.29974086  1.        ]\n",
            " [ 1.90249958  1.        ]\n",
            " [ 1.95011718  1.        ]\n",
            " [-2.12489791  1.        ]\n",
            " [ 1.68144     1.        ]\n",
            " [-3.43847888  1.        ]\n",
            " [-3.56085957  1.        ]\n",
            " [-3.13504232  1.        ]\n",
            " [-2.49988996  1.        ]\n",
            " [-0.20600208  1.        ]\n",
            " [ 5.88494294  1.        ]\n",
            " [ 2.36320375  1.        ]\n",
            " [-1.19986118  1.        ]\n",
            " [ 0.50218804  1.        ]\n",
            " [ 1.92931341  1.        ]\n",
            " [-1.47447163  1.        ]\n",
            " [-1.83841669  1.        ]\n",
            " [-3.7966465   1.        ]\n",
            " [ 5.7520238   1.        ]\n",
            " [-0.77114012  1.        ]\n",
            " [-0.65554672  1.        ]\n",
            " [-1.46369856  1.        ]\n",
            " [ 1.02482853  1.        ]\n",
            " [-0.30169725  1.        ]\n",
            " [-3.89903629  1.        ]\n",
            " [-1.65955269  1.        ]\n",
            " [-3.85666712  1.        ]\n",
            " [ 1.16760011  1.        ]\n",
            " [-2.81953827  1.        ]\n",
            " [-2.5266864   1.        ]\n",
            " [-0.86667983  1.        ]\n",
            " [ 1.08199022  1.        ]\n",
            " [ 0.11122154  1.        ]\n",
            " [ 0.34873964  1.        ]\n",
            " [ 0.48930775  1.        ]\n",
            " [ 1.68293474  1.        ]\n",
            " [-2.12171285  1.        ]\n",
            " [-1.74256899  1.        ]\n",
            " [-3.79572651  1.        ]\n",
            " [-2.28720863  1.        ]\n",
            " [-1.06930464  1.        ]\n",
            " [-1.22055978  1.        ]\n",
            " [ 0.555611    1.        ]\n",
            " [ 0.98764901  1.        ]\n",
            " [-3.0129564   1.        ]\n",
            " [-2.60860073  1.        ]\n",
            " [-0.67765212  1.        ]\n",
            " [ 1.30712919  1.        ]\n",
            " [ 7.1940367   1.        ]\n",
            " [-3.01755007  1.        ]\n",
            " [-3.54873333  1.        ]\n",
            " [ 6.23830444  1.        ]\n",
            " [-1.25152264  1.        ]\n",
            " [-2.7927808   1.        ]\n",
            " [-0.40285316  1.        ]\n",
            " [ 1.13547377  1.        ]\n",
            " [-1.08877623  1.        ]\n",
            " [-3.96505243  1.        ]\n",
            " [-3.6494653   1.        ]\n",
            " [ 0.24952877  1.        ]\n",
            " [-2.31754745  1.        ]\n",
            " [ 6.02181143  1.        ]\n",
            " [-1.34326666  1.        ]\n",
            " [ 0.40154277  1.        ]\n",
            " [ 0.02183988  1.        ]\n",
            " [ 1.76994316  1.        ]\n",
            " [-2.97876295  1.        ]\n",
            " [-1.62246303  1.        ]\n",
            " [-0.49020251  1.        ]\n",
            " [-3.13598275  1.        ]\n",
            " [ 0.53132946  1.        ]\n",
            " [ 0.64336579  1.        ]\n",
            " [-0.75263887  1.        ]\n",
            " [-3.98351711  1.        ]\n",
            " [-1.96483064  1.        ]\n",
            " [ 1.83013985  1.        ]\n",
            " [ 1.64560513  1.        ]\n",
            " [-2.72101194  1.        ]\n",
            " [ 0.80002381  1.        ]\n",
            " [-0.90358849  1.        ]\n",
            " [-0.55814001  1.        ]\n",
            " [-1.89385682  1.        ]\n",
            " [-0.63702134  1.        ]\n",
            " [ 0.99856861  1.        ]\n",
            " [-3.00050094  1.        ]\n",
            " [ 1.78980646  1.        ]\n",
            " [-2.14619081  1.        ]\n",
            " [-2.34212192  1.        ]\n",
            " [-0.6607631   1.        ]\n",
            " [-3.44204804  1.        ]\n",
            " [ 0.35176363  1.        ]\n",
            " [-1.36038112  1.        ]\n",
            " [-2.27430639  1.        ]\n",
            " [-3.60662109  1.        ]\n",
            " [-1.90369385  1.        ]\n",
            " [-2.4986816   1.        ]\n",
            " [ 1.16770098  1.        ]\n",
            " [ 1.49184961  1.        ]\n",
            " [ 0.86164611  1.        ]\n",
            " [ 4.6557008   1.        ]\n",
            " [-1.56406737  1.        ]\n",
            " [-1.29376668  1.        ]\n",
            " [ 0.64798481  1.        ]\n",
            " [-1.76887258  1.        ]\n",
            " [-2.43781467  1.        ]\n",
            " [-0.9771823   1.        ]\n",
            " [ 0.74993723  1.        ]\n",
            " [-2.27003207  1.        ]\n",
            " [ 5.28397811  1.        ]\n",
            " [-0.16401172  1.        ]\n",
            " [-3.77824181  1.        ]\n",
            " [ 0.18736142  1.        ]\n",
            " [-0.01359375  1.        ]\n",
            " [-2.87923792  1.        ]\n",
            " [-2.10004935  1.        ]\n",
            " [ 1.43001542  1.        ]\n",
            " [-1.60425392  1.        ]\n",
            " [-0.12567181  1.        ]\n",
            " [ 1.11579703  1.        ]\n",
            " [-1.50148031  1.        ]\n",
            " [ 0.72076857  1.        ]\n",
            " [-2.69938311  1.        ]\n",
            " [ 0.44566672  1.        ]\n",
            " [ 0.81418497  1.        ]\n",
            " [-1.45321849  1.        ]\n",
            " [-3.60820774  1.        ]\n",
            " [-1.16854168  1.        ]\n",
            " [-3.76244274  1.        ]\n",
            " [-0.21809347  1.        ]\n",
            " [-1.00608129  1.        ]\n",
            " [-3.50885202  1.        ]\n",
            " [-3.11246779  1.        ]\n",
            " [-3.20382221  1.        ]\n",
            " [ 0.96621556  1.        ]\n",
            " [-0.20104032  1.        ]\n",
            " [-3.0738782   1.        ]\n",
            " [-3.68694245  1.        ]\n",
            " [ 1.78202524  1.        ]\n",
            " [ 1.77042049  1.        ]\n",
            " [-2.72995154  1.        ]\n",
            " [ 6.92308208  1.        ]\n",
            " [-3.89966307  1.        ]\n",
            " [ 0.91776031  1.        ]\n",
            " [ 1.92463606  1.        ]\n",
            " [ 0.83621425  1.        ]\n",
            " [-3.0486698   1.        ]\n",
            " [ 1.84480658  1.        ]\n",
            " [ 6.73146265  1.        ]\n",
            " [-0.41807362  1.        ]\n",
            " [-3.84109271  1.        ]\n",
            " [ 5.28107201  1.        ]\n",
            " [-3.19519615  1.        ]\n",
            " [ 1.49321842  1.        ]\n",
            " [ 0.35884166  1.        ]\n",
            " [-1.36340404  1.        ]\n",
            " [-3.72080079  1.        ]\n",
            " [-0.92907025  1.        ]\n",
            " [-1.79380258  1.        ]\n",
            " [ 5.24977784  1.        ]\n",
            " [ 1.47609974  1.        ]\n",
            " [-1.26442622  1.        ]\n",
            " [-1.65157558  1.        ]\n",
            " [ 0.83809361  1.        ]\n",
            " [-3.62891839  1.        ]\n",
            " [-0.62769046  1.        ]\n",
            " [-3.72407692  1.        ]\n",
            " [ 3.57883     1.        ]\n",
            " [-2.1500863   1.        ]\n",
            " [-0.73798426  1.        ]\n",
            " [-3.2104387   1.        ]\n",
            " [-0.15993665  1.        ]\n",
            " [-1.54480518  1.        ]\n",
            " [-2.43565167  1.        ]\n",
            " [ 0.04075932  1.        ]\n",
            " [ 0.61263056  1.        ]\n",
            " [-2.3606993   1.        ]\n",
            " [-2.11519663  1.        ]\n",
            " [-3.68995637  1.        ]\n",
            " [-3.66476821  1.        ]\n",
            " [ 0.96095198  1.        ]\n",
            " [-3.34159202  1.        ]\n",
            " [ 0.0295258   1.        ]\n",
            " [-3.88099513  1.        ]\n",
            " [ 1.04736796  1.        ]\n",
            " [ 0.72258165  1.        ]\n",
            " [-0.36590111  1.        ]\n",
            " [ 1.66937546  1.        ]\n",
            " [-2.72301403  1.        ]\n",
            " [-3.23427588  1.        ]\n",
            " [-3.44746621  1.        ]\n",
            " [-0.38905931  1.        ]\n",
            " [-0.12257862  1.        ]\n",
            " [ 0.47569028  1.        ]\n",
            " [-2.95369658  1.        ]\n",
            " [-0.36095146  1.        ]\n",
            " [-2.42711922  1.        ]\n",
            " [-3.23911151  1.        ]\n",
            " [-2.9330453   1.        ]\n",
            " [ 0.830329    1.        ]\n",
            " [ 0.77927666  1.        ]\n",
            " [-1.94923166  1.        ]\n",
            " [-1.71745438  1.        ]\n",
            " [ 1.82959048  1.        ]\n",
            " [-0.36918046  1.        ]\n",
            " [-3.00276901  1.        ]\n",
            " [-1.47756457  1.        ]\n",
            " [ 5.33881459  1.        ]\n",
            " [-1.34842353  1.        ]\n",
            " [-2.22859447  1.        ]\n",
            " [-1.71411076  1.        ]\n",
            " [ 0.09972144  1.        ]\n",
            " [-3.95744903  1.        ]\n",
            " [-3.70309729  1.        ]\n",
            " [-0.46298763  1.        ]\n",
            " [-2.95098677  1.        ]\n",
            " [ 1.79997437  1.        ]\n",
            " [ 1.77718747  1.        ]\n",
            " [ 0.69656281  1.        ]\n",
            " [-3.44971158  1.        ]\n",
            " [ 8.17987483  1.        ]\n",
            " [-2.33290953  1.        ]\n",
            " [-2.4587952   1.        ]\n",
            " [-1.14030048  1.        ]\n",
            " [ 5.49511218  1.        ]\n",
            " [ 1.0134153   1.        ]\n",
            " [-3.86429363  1.        ]\n",
            " [ 0.14802221  1.        ]\n",
            " [ 1.71217561  1.        ]\n",
            " [-2.85030245  1.        ]\n",
            " [-1.64504934  1.        ]\n",
            " [-0.48089004  1.        ]\n",
            " [-2.20110675  1.        ]\n",
            " [-0.12567463  1.        ]\n",
            " [ 1.7790903   1.        ]\n",
            " [ 1.51453172  1.        ]\n",
            " [-1.62666147  1.        ]\n",
            " [-0.09596542  1.        ]\n",
            " [ 0.54323796  1.        ]\n",
            " [-2.17894644  1.        ]\n",
            " [ 5.45930528  1.        ]\n",
            " [-0.07680393  1.        ]\n",
            " [-0.14229932  1.        ]\n",
            " [ 7.78478936  1.        ]\n",
            " [ 0.13778093  1.        ]\n",
            " [ 0.61110199  1.        ]\n",
            " [-0.44235389  1.        ]\n",
            " [-1.20440055  1.        ]\n",
            " [ 1.50378623  1.        ]\n",
            " [ 1.20822857  1.        ]\n",
            " [-2.30873304  1.        ]\n",
            " [ 6.735317    1.        ]\n",
            " [ 7.35101315  1.        ]\n",
            " [-0.99786142  1.        ]\n",
            " [-2.12901129  1.        ]\n",
            " [-3.83464532  1.        ]\n",
            " [ 1.72490802  1.        ]\n",
            " [ 3.84801155  1.        ]\n",
            " [-1.51209903  1.        ]\n",
            " [-2.099571    1.        ]\n",
            " [-1.10672567  1.        ]\n",
            " [-1.42321513  1.        ]\n",
            " [-2.32739214  1.        ]\n",
            " [-1.74911802  1.        ]\n",
            " [-1.46090803  1.        ]\n",
            " [-1.85523552  1.        ]\n",
            " [-1.68647646  1.        ]\n",
            " [-3.39716626  1.        ]\n",
            " [-2.43941531  1.        ]\n",
            " [-0.33052162  1.        ]]\n",
            "shape of xtrain_ext\n",
            "(375, 2)\n",
            "xtest_ext:\n",
            "[[-2.33865278  1.        ]\n",
            " [-1.80493882  1.        ]\n",
            " [-1.82381886  1.        ]\n",
            " [-0.54185159  1.        ]\n",
            " [-0.15091846  1.        ]\n",
            " [-2.92856873  1.        ]\n",
            " [-3.62369246  1.        ]\n",
            " [ 8.06138089  1.        ]\n",
            " [-0.66411377  1.        ]\n",
            " [ 4.76830383  1.        ]\n",
            " [ 6.08010598  1.        ]\n",
            " [ 5.09012288  1.        ]\n",
            " [-2.42611996  1.        ]\n",
            " [ 0.71051176  1.        ]\n",
            " [ 1.555785    1.        ]\n",
            " [ 0.88338695  1.        ]\n",
            " [ 1.21920436  1.        ]\n",
            " [-2.85205644  1.        ]\n",
            " [ 1.50615354  1.        ]\n",
            " [-2.81590681  1.        ]\n",
            " [ 0.95734008  1.        ]\n",
            " [-3.17306334  1.        ]\n",
            " [-1.68605293  1.        ]\n",
            " [-0.31663003  1.        ]\n",
            " [-1.69590749  1.        ]\n",
            " [-0.80004295  1.        ]\n",
            " [-1.46004131  1.        ]\n",
            " [-1.67961656  1.        ]\n",
            " [ 1.25922258  1.        ]\n",
            " [-3.80751191  1.        ]\n",
            " [-2.708691    1.        ]\n",
            " [-3.2648461   1.        ]\n",
            " [-3.60760621  1.        ]\n",
            " [-2.87305154  1.        ]\n",
            " [ 0.3194563   1.        ]\n",
            " [ 1.79496872  1.        ]\n",
            " [-3.29136864  1.        ]\n",
            " [-2.35570967  1.        ]\n",
            " [ 1.78676804  1.        ]\n",
            " [-1.68490604  1.        ]\n",
            " [-2.77655572  1.        ]\n",
            " [ 1.03928913  1.        ]\n",
            " [-0.82846442  1.        ]\n",
            " [ 4.27680766  1.        ]\n",
            " [-2.78055074  1.        ]\n",
            " [-1.43409369  1.        ]\n",
            " [-3.19574982  1.        ]\n",
            " [-2.34287727  1.        ]\n",
            " [ 1.07013855  1.        ]\n",
            " [-0.91919306  1.        ]\n",
            " [-3.76875166  1.        ]\n",
            " [-3.62590253  1.        ]\n",
            " [-1.03857064  1.        ]\n",
            " [-1.59018266  1.        ]\n",
            " [ 0.83269171  1.        ]\n",
            " [-1.59313503  1.        ]\n",
            " [-1.71317481  1.        ]\n",
            " [-1.08885415  1.        ]\n",
            " [-0.23831201  1.        ]\n",
            " [ 0.14777241  1.        ]\n",
            " [ 0.5787712   1.        ]\n",
            " [-0.77816752  1.        ]\n",
            " [-2.73160931  1.        ]\n",
            " [-2.28105385  1.        ]\n",
            " [ 5.45171106  1.        ]\n",
            " [ 0.52107865  1.        ]\n",
            " [-3.37550179  1.        ]\n",
            " [ 4.98374009  1.        ]\n",
            " [-1.31912229  1.        ]\n",
            " [ 5.47054347  1.        ]\n",
            " [ 0.04813637  1.        ]\n",
            " [-0.15203099  1.        ]\n",
            " [ 1.84815688  1.        ]\n",
            " [-1.63764872  1.        ]\n",
            " [ 0.73057222  1.        ]\n",
            " [ 0.66009869  1.        ]\n",
            " [-0.26143726  1.        ]\n",
            " [-0.01009452  1.        ]\n",
            " [-1.13698314  1.        ]\n",
            " [ 1.3131216   1.        ]\n",
            " [-2.0216097   1.        ]\n",
            " [-2.42503172  1.        ]\n",
            " [-1.83288792  1.        ]\n",
            " [ 0.99821047  1.        ]\n",
            " [ 1.09426713  1.        ]\n",
            " [-3.253238    1.        ]\n",
            " [-0.57661241  1.        ]\n",
            " [-3.05648867  1.        ]\n",
            " [ 0.56473271  1.        ]\n",
            " [-3.36447258  1.        ]\n",
            " [ 0.30423036  1.        ]\n",
            " [ 0.52019251  1.        ]\n",
            " [-3.50772996  1.        ]\n",
            " [ 4.597577    1.        ]\n",
            " [-1.7110424   1.        ]\n",
            " [-3.09685102  1.        ]\n",
            " [ 5.03962554  1.        ]\n",
            " [ 0.96557098  1.        ]\n",
            " [ 0.54983517  1.        ]\n",
            " [-2.74431374  1.        ]\n",
            " [-3.0619201   1.        ]\n",
            " [ 4.58595523  1.        ]\n",
            " [-3.17686027  1.        ]\n",
            " [-0.25906147  1.        ]\n",
            " [-3.0362582   1.        ]\n",
            " [-2.49239453  1.        ]\n",
            " [-2.22996134  1.        ]\n",
            " [ 3.46733963  1.        ]\n",
            " [ 1.13136185  1.        ]\n",
            " [-2.82221869  1.        ]\n",
            " [ 0.26821426  1.        ]\n",
            " [-2.74002397  1.        ]\n",
            " [ 0.51818125  1.        ]\n",
            " [-2.41396163  1.        ]\n",
            " [ 0.29632397  1.        ]\n",
            " [-0.96361046  1.        ]\n",
            " [ 0.84453669  1.        ]\n",
            " [-0.24843109  1.        ]\n",
            " [-1.03596547  1.        ]\n",
            " [-2.53904373  1.        ]\n",
            " [-1.13755597  1.        ]\n",
            " [ 1.99415529  1.        ]\n",
            " [ 2.95167164  1.        ]\n",
            " [-1.30962214  1.        ]\n",
            " [-2.29716107  1.        ]]\n",
            "shape of xtest_ext\n",
            "(125, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7z43OcK2ax"
      },
      "source": [
        "## **Question 2:** OLS Regression for Binary Classification (20 points)\n",
        "\n",
        "In class, we talked about one way to implement clasification is to use one of the linear regression methods we learned. We will investigate how to use OLS for binary classification and see how it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZX2J8wLVix"
      },
      "source": [
        "### Problem a. (3 points)\n",
        "\n",
        "When OLS takes a feature vector, it gives you a real-valued scalar. Assuming OLS works well (i.e., it gives a value close to the binary label (either -1 or 1) for most of the samples), how can you translate the value into the label? Suggest a simple method.\n",
        "\n",
        "Hint: we talked about this in class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdm7yHm72aa"
      },
      "source": [
        "Most easily, you take the sign of the value. If sign y(x) is greater than or equal to 0, assign to class 1 with label y= -1. If sign y(x) is less than 0, assign to class 2 with label y= +1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEFD30vtfu3T"
      },
      "source": [
        "### Problem b. (7 points)\n",
        "For simplicity, we will use the OLS implementation from sklearn whose documentation can be found here:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "\n",
        "Fit the model to X_train, y_train and then extract w and b. You should do so by using get_wOLS_ext, which you need to complete. Note that the function should return a single vector, which is the regular w vector with the bias term appended to it.\n",
        "\n",
        "Lastly, create the following plot:\n",
        "* a scatter plot of the training data overlayed by the OLS solution. For this, You can assume that X_train has only one feature.\n",
        "\n",
        "Hint: w is the coeficient and b is the intercept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvgEoJUMcpEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "a0f3cee4-bcc1-4cac-ba75-2e7ad19192e3"
      },
      "source": [
        "from sklearn import linear_model\n",
        "# WRITE CODE HERE: \n",
        "# DO NOT assume that X has only one feature\n",
        "def get_wOLS_ext(X, y):\n",
        "    reg = linear_model.LinearRegression().fit(X, y)\n",
        "    w = []\n",
        "    w = reg.coef_\n",
        "    w = np.append(w, reg.intercept_)\n",
        "    print(w.shape)\n",
        "    return w\n",
        "\n",
        "wOLS_ext = get_wOLS_ext(xtrain, ytrain)\n",
        "print(wOLS_ext)\n",
        "\n",
        "### ADD CODE HERE:\n",
        "### Use plt.plot() to plot the model curve\n",
        "line = xtrain * wOLS_ext[0] + wOLS_ext[1]\n",
        "plt.plot(xtrain, line,color='orange')\n",
        "### Use plt.scatter() to plot the original feature data\n",
        "plt.scatter(xtrain, ytrain, marker='o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "[ 0.15042363 -0.76859499]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fc0180e2e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhgVuFW9w773"
      },
      "source": [
        "### Problem c. (6 points)\n",
        "\n",
        "Complete the linear_binary_predict function to predict the labels of the test set using the OLS solution from part a. Also complete compute_CCR and report the correct classification rate (CCR).\n",
        "\n",
        "linear_binary_predict takes Xext, a matrix of extended feature vectors, and wext, an extended weight vector; it performs linear binary classification and returns a vector of predicted binary labels.\n",
        "\n",
        "compute_CCR takes Xext, wext, along with y, the expected binary labels to compute CCR. It should make use of linear_binary_predict.\n",
        "\n",
        "Note: \n",
        "*   We assume Xext to be row-major, meaning each row is a sample.\n",
        "*   Do not change the function prototype of linear_binary_predict. This also applies to all other functions we ask you to complete\n",
        "*   The OLS from sklearn has a builtin predict function. We are asking you to complete linear_binary_predict instead of using the builtin predict because linear_binary_predict will be important for later parts.\n",
        "\n",
        "Hint: use the idea from part a.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrhLQ-0_x8OX",
        "outputId": "97880431-811a-4062-86c6-ccf2b945ffa6"
      },
      "source": [
        "# WRITE CODE HERE:\n",
        "def linear_binary_predict(Xext, wext):\n",
        "   # s1 = np.size(Xext)\n",
        "\n",
        "    if Xext.shape[1] <= 2:\n",
        "      ypred = np.dot(Xext,np.transpose(wext[0])) + wext[1]\n",
        "    else:\n",
        "   #   Xext = Xext[:,:-1]\n",
        "      ypred = np.matmul(Xext, wext)\n",
        "      \n",
        "\n",
        "    #print(Xext.shape)\n",
        "    #print(wext.shape)\n",
        "  #  signvec = np.ones(s1)\n",
        "\n",
        "    # if np.sign(ypred) < 0:\n",
        "    #     signvec[i] *= -1\n",
        "    # elif ypred == 0:\n",
        "    #     signvec[i] *= -1\n",
        "    return np.sign(ypred)\n",
        "\n",
        "def compute_CCR(Xext, wext, y):\n",
        "    #s1 = np.size(Xext)\n",
        "    pred_labels = linear_binary_predict(Xext, wext)\n",
        "    #sign = 0\n",
        "    frac_correct = np.mean(pred_labels == y)\n",
        "    return frac_correct\n",
        "    # for i in range(s1):\n",
        "    #   if signvec[i] == y[i]:\n",
        "    #     sign += 1\n",
        "   # return sign/s1\n",
        "\n",
        "CCR = compute_CCR(Xte_ext, wOLS_ext, ytest)\n",
        "print(\"The test CCR using OLS is\", CCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using OLS is 0.912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX-NCA9sZ5XO"
      },
      "source": [
        "### Problem d. (4 points)\n",
        "Explain why CCR is not a good metric in this case. Suggest an alternative that better captures the performance of OLS on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZmVPImh-XSk"
      },
      "source": [
        "CCR becomes a bad metric when the data isn't balanced. Methods like LDA/Fisher's Linear Discriminant uses covariances matrices to evaluate shared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF7u7IoGC96w"
      },
      "source": [
        "## **Question 3:** Fisher's Linear Discriminant (27 points)\n",
        "Fisher's Linear Discriminant is a method that takes d dimensional feature vectors and projects them into 1 dimension, and it tries to do so in a fashion where the resulting 1D values are well-separated by class. Here our d happens to be 1, but as mentioned above, the code you write needs to be able to accomodate higher dimensions for full credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaVLlwwJWrpX"
      },
      "source": [
        "### a. Separate the training set by class (4 points)\n",
        "\n",
        "Write a function seperate that takes the inputs X, y and separates X based on y.\n",
        "\n",
        "For full credit, create a vectorized implementation (no for loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991ZOOYCY02l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb99116-de75-4826-a0a8-0e746d5fec17"
      },
      "source": [
        "def seperate(X, y):\n",
        "    # WRITE CODE HERE:\n",
        "    X1 = X[y<0]\n",
        "    X2 = X[y>0]\n",
        "    # X1 should be of the negative class, and X2 the positive class\n",
        "    return X1, X2\n",
        "\n",
        "X1, X2 = seperate(xtrain, ytrain)\n",
        "print(X1)\n",
        "print(X1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.54070721 -2.67418502 -3.10571186 -0.5398017  -0.26611982 -2.30097366\n",
            " -0.32198019 -1.16351213 -2.17789568 -1.47286711 -0.6250596  -0.90906818\n",
            "  1.7437662  -3.05271398 -0.94302471 -2.81563449  0.39416502 -1.0092425\n",
            " -1.87989533 -3.30480633 -2.36928921  1.68203701 -2.06749188  1.20392303\n",
            " -1.2439847   0.95292688  1.54389875 -1.97287478 -3.20692695 -0.93466669\n",
            " -0.16316838 -0.74958184  1.24664721  1.50489064 -1.27901266 -3.52034497\n",
            "  0.16173701  1.70278218 -3.53749715 -0.69809049  0.15717339  0.87649786\n",
            "  1.97684714 -2.33719999 -0.54576904 -2.8492077  -2.77926856 -1.41263951\n",
            "  1.8844232   1.56083121 -2.6649588  -0.10689548 -0.75315368 -0.20948685\n",
            " -0.27807667 -3.32095504  0.8746826  -3.85305594 -1.4874819   1.69189668\n",
            " -3.02627599 -1.67338531 -3.87121674 -1.7422162  -0.20718929 -2.45318129\n",
            "  0.94689286 -2.18083102  0.67148221 -1.80995642 -3.98765894 -3.5106843\n",
            "  1.63251097 -0.03786772 -3.03608795 -0.82466315 -2.70127852 -3.22796073\n",
            " -0.74011953 -1.12579891  1.81955248  0.05406853 -0.31732865  1.36829517\n",
            " -0.9381805   1.03810762 -3.58954219 -3.41370605  0.35854784 -1.84127844\n",
            "  0.08863072 -0.84622286 -0.87084927  1.87703747 -1.58132208 -0.75993277\n",
            " -2.39685049 -0.70243787  0.98750665  0.74680369 -3.29974086  1.90249958\n",
            "  1.95011718 -2.12489791  1.68144    -3.43847888 -3.56085957 -3.13504232\n",
            " -2.49988996 -0.20600208 -1.19986118  0.50218804  1.92931341 -1.47447163\n",
            " -1.83841669 -3.7966465  -0.77114012 -0.65554672 -1.46369856  1.02482853\n",
            " -0.30169725 -3.89903629 -1.65955269 -3.85666712  1.16760011 -2.81953827\n",
            " -2.5266864  -0.86667983  1.08199022  0.11122154  0.34873964  0.48930775\n",
            "  1.68293474 -2.12171285 -1.74256899 -3.79572651 -2.28720863 -1.06930464\n",
            " -1.22055978  0.555611    0.98764901 -3.0129564  -2.60860073 -0.67765212\n",
            "  1.30712919 -3.01755007 -3.54873333 -1.25152264 -2.7927808  -0.40285316\n",
            "  1.13547377 -1.08877623 -3.96505243 -3.6494653   0.24952877 -2.31754745\n",
            " -1.34326666  0.40154277  0.02183988  1.76994316 -2.97876295 -1.62246303\n",
            " -0.49020251 -3.13598275  0.53132946  0.64336579 -0.75263887 -3.98351711\n",
            " -1.96483064  1.83013985  1.64560513 -2.72101194  0.80002381 -0.90358849\n",
            " -0.55814001 -1.89385682 -0.63702134  0.99856861 -3.00050094  1.78980646\n",
            " -2.14619081 -2.34212192 -0.6607631  -3.44204804  0.35176363 -1.36038112\n",
            " -2.27430639 -3.60662109 -1.90369385 -2.4986816   1.16770098  1.49184961\n",
            "  0.86164611 -1.56406737 -1.29376668  0.64798481 -1.76887258 -2.43781467\n",
            " -0.9771823   0.74993723 -2.27003207 -0.16401172 -3.77824181  0.18736142\n",
            " -0.01359375 -2.87923792 -2.10004935  1.43001542 -1.60425392 -0.12567181\n",
            "  1.11579703 -1.50148031  0.72076857 -2.69938311  0.44566672  0.81418497\n",
            " -1.45321849 -3.60820774 -1.16854168 -3.76244274 -0.21809347 -1.00608129\n",
            " -3.50885202 -3.11246779 -3.20382221  0.96621556 -0.20104032 -3.0738782\n",
            " -3.68694245  1.78202524  1.77042049 -2.72995154 -3.89966307  0.91776031\n",
            "  1.92463606  0.83621425 -3.0486698   1.84480658 -0.41807362 -3.84109271\n",
            " -3.19519615  1.49321842  0.35884166 -1.36340404 -3.72080079 -0.92907025\n",
            " -1.79380258  1.47609974 -1.26442622 -1.65157558  0.83809361 -3.62891839\n",
            " -0.62769046 -3.72407692 -2.1500863  -0.73798426 -3.2104387  -0.15993665\n",
            " -1.54480518 -2.43565167  0.04075932  0.61263056 -2.3606993  -2.11519663\n",
            " -3.68995637 -3.66476821  0.96095198 -3.34159202  0.0295258  -3.88099513\n",
            "  1.04736796  0.72258165 -0.36590111  1.66937546 -2.72301403 -3.23427588\n",
            " -3.44746621 -0.38905931 -0.12257862  0.47569028 -2.95369658 -0.36095146\n",
            " -2.42711922 -3.23911151 -2.9330453   0.830329    0.77927666 -1.94923166\n",
            " -1.71745438  1.82959048 -0.36918046 -3.00276901 -1.47756457 -1.34842353\n",
            " -2.22859447 -1.71411076  0.09972144 -3.95744903 -3.70309729 -0.46298763\n",
            " -2.95098677  1.79997437  1.77718747  0.69656281 -3.44971158 -2.33290953\n",
            " -2.4587952  -1.14030048  1.0134153  -3.86429363  0.14802221  1.71217561\n",
            " -2.85030245 -1.64504934 -0.48089004 -2.20110675 -0.12567463  1.7790903\n",
            "  1.51453172 -1.62666147 -0.09596542  0.54323796 -2.17894644 -0.07680393\n",
            " -0.14229932  0.13778093  0.61110199 -0.44235389 -1.20440055  1.50378623\n",
            "  1.20822857 -2.30873304 -0.99786142 -2.12901129 -3.83464532  1.72490802\n",
            " -1.51209903 -2.099571   -1.10672567 -1.42321513 -2.32739214 -1.74911802\n",
            " -1.46090803 -1.85523552 -1.68647646 -3.39716626 -2.43941531 -0.33052162]\n",
            "(348,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMMqIr0xB9BP"
      },
      "source": [
        "### b. Calculate mean vectors (4 points)\n",
        "Write a function get_means that takes the inputs X1, X2 and calculates the mean vectors of the two classes.\n",
        "\n",
        "For full credit, create a vectorized implementation (no for loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuPgvLQKTPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd32593-4f2c-44cd-bc1c-2253086b71d2"
      },
      "source": [
        "def get_means(X1, X2):\n",
        "    # WRITE CODE HERE:\n",
        "    #m1 = np.mean(X1)\n",
        "    print(X1.ndim)\n",
        "    #if X1.ndim == 1:\n",
        "    m1 = np.mean(X1)\n",
        "    m2 = np.mean(X2)\n",
        "    #else:\n",
        "    #  m1 = np.mean(X1[:,:-1])\n",
        "    #  m2 = np.mean(X2[:,:-1])\n",
        "    #m1 = np.mean(X1,axis=1).reshape(2,1)\n",
        "    \n",
        "    #m2 = np.mean(X2,axis=1).reshape(2,1)\n",
        "    return m1, m2 \n",
        "\n",
        "m1, m2 = get_means(X1, X2)\n",
        "print(m1)\n",
        "print(m2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "-1.0708543196085778\n",
            "5.731857708252586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3xQqSq_Ogmd"
      },
      "source": [
        "### c. Calculate within-class covariance (5 points)\n",
        "$S_w$, the total within-class covariance matrix, is a d by d matrix given by $S_1 + S_2$ where $S_1 = \\sum_{x_i \\in X_1}(x_i-m_1)^T(x_i-m_1) = (X_1-m_1)^T(X_1-m_1)$. \n",
        "\n",
        "Write a function get_Sw that takes the inputs X1, X2, m1, m2 and calculates Sw.\n",
        "\n",
        "For full credit, create a vectorized implementation.\n",
        "\n",
        "**Note:** depending on the schema of the dataset (row-major vs column-major), the formula to use might differ slightly from those on the lecture slides. We assume row-major (each sample is a row) here, as many public datasets are organized this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5NaX0uwUXll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b71b80-80d3-482b-b2d8-c4c4a0ab3185"
      },
      "source": [
        "def get_Sw(X1, X2, m1, m2):\n",
        "    # WRITE CODE HERE:\n",
        "    s1 = np.matmul(np.transpose(X1-m1), (X1-m1))\n",
        "    s2 = np.matmul(np.transpose(X2-m2), (X2-m2))\n",
        "    return [s1+s2]\n",
        "\n",
        "Sw = get_Sw(X1, X2, m1, m2)\n",
        "print(Sw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1106.7359470871195]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnV8WWDBdZAG"
      },
      "source": [
        "### d. Calculate $w_{FLD}$ and $b_{FLD}$ (5 points)\n",
        "Write a function get_wFLD_ext that takes the inputs Sw, m1, m2 and calculates the extended $w_{FLD}$.\n",
        "\n",
        "Recall that in class we mentioned the average of the averages of the two classes after the linear transformation can serve as the bias term. However, since we want the outputs of $Xw + b$ on the two classes to be roughly separated by 0, we actually want the effect of adding b to be the same as subtracting that average. Thus, use $b = -(m_1w + m_2w)/2$.\n",
        "\n",
        "For full credit, create a vectorized implementation, and do not use the inverse function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1BuMyNvdXm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055f1347-6e51-4536-d319-188e202af55c"
      },
      "source": [
        "def get_wFLD_ext(Sw, m1, m2):\n",
        "    # WRITE CODE HERE: \n",
        "    diff = m2-m1\n",
        "    #sz = np.size(Sw)\n",
        "    #i = np.identity(sz)\n",
        "    #prod = i*diff\n",
        "    if m1.ndim == 0:\n",
        "      w = Sw / diff\n",
        "    else:\n",
        "      w = np.linalg.solve(Sw, diff)\n",
        "    print(\"DIM M1\")\n",
        "    print(m1.ndim)\n",
        "    if m1.ndim == 0:\n",
        "      b = -(np.dot(m1,w)+np.dot(m2,w))/2\n",
        "    else:\n",
        "      b = -(np.matmul(m1,w)+np.matmul(m2,w))/2\n",
        "    wFLD_ext = np.append(w,b)\n",
        "    return wFLD_ext\n",
        "\n",
        "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
        "print(wFLD_ext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DIM M1\n",
            "0\n",
            "[ 162.69040091 -379.15025497]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5FyLF32e9t0"
      },
      "source": [
        "### e. Evaluation (4 points)\n",
        "\n",
        "First, create a plot of the FLD solution using the training set. Then use the compute_CCR function that you wrote earlier to get the FLD CCR on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "6E8A6SAwhTzy",
        "outputId": "33ca1b80-6c07-4f02-a739-09da930f4fb7"
      },
      "source": [
        "# WRITE CODE HERE: \n",
        "\n",
        "CCR = compute_CCR(Xte_ext, wFLD_ext, ytest)\n",
        "print(\"The test CCR using FLD is\", CCR)\n",
        "line = wFLD_ext[0]*Xtr_ext + wFLD_ext[1]\n",
        "plt.plot(Xtr_ext, line, color = 'b')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using FLD is 0.948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc01806c790>,\n",
              " <matplotlib.lines.Line2D at 0x7fc01806c990>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcN0lEQVR4nO3de3TV1Z338fdXEBBQgXIRCRbmKU4HW0XNiPNoZ2q1CNop2o6M2j6lrYodr7XOesTa2kddVavWGyIW8YIWRURZRkXDxaJ1BCGIIhCRFA3hHi4CXrl9nz/27/QkyCEJyTm/3znn81orK9k7J+G7NPDJd+/fb//M3REREdmbA+IuQEREkkshISIiGSkkREQkI4WEiIhkpJAQEZGMWsddQHN17drV+/TpE3cZIiJ5Zf78+RvcvVtDr8v7kOjTpw8VFRVxlyEiklfMrLoxr9Nyk4iIZKSQEBGRjBQSIiKSkUJCREQyUkiIiEhGCgkREclIISEiIhkpJERE8syUKTB5cm7+rLy/mU5EpFh8/DEccgikHgO0ezeYZffPVCchIpIH7rsPDj44HRCVldkPCFAnISKSaBs2QLc6Jyz94hcwZkzu/nx1EiIiCXX99fUDoqYmtwEBCgkRkcRZsSIsJd10UxjfcENYZiopyX0tWm4SEUmQESPgwQfT4w0b4Ctfia8edRIiIgmwZEnoHlIBMWZM6B7iDAhQJyEiEit3+Pd/hxdfDOMDD4TNm6FDh3jrSlEnISISk9mz4YAD0gExaRJs356cgAB1EiIiObdrFxx/PLzzThj37QtLl4YuImnUSYiI5NBLL0Hr1umAmDEDli9PZkBAC4WEmT1sZuvNbFGduS5mNt3MlkXvO0fzZmb3mlmVmS00s+PqfM3w6PXLzGx4S9QmIpIEX3wB3bvDGWeE8UknhY7i1FPjrashLdVJPAoM3mNuJDDT3fsBM6MxwBCgX/Q2AhgDIVSA3wEDgROA36WCRUQkn02YAO3aQW1tGM+bB6+/HvYjkq5FSnT314BNe0wPBcZHH48Hzqoz/5gHc4BOZtYTOB2Y7u6b3H0zMJ0vB4+ISN7YujVc1vrjH4fxOeeEQ/lKS+OtqymymWM93H1N9PFaoEf0cS+gps7rVkZzmea/xMxGmFmFmVXUpqJZRCRB7r4bDj00PV66NFy9lItD+VpSTpodd3fAW/D7jXX3Uncv7Vb3YBMRkZitXx+C4KqrwviKK8K9EEceGW9d+yubIbEuWkYier8+ml8F9K7zupJoLtO8iEheuPZa6NEjPV61Cu65J756WkI2Q6IMSF2hNBx4rs78T6KrnE4EtkTLUuXAIDPrHG1YD4rmREQS7cMPQ/dw661h/Pvfh+7h8MNjLatFtMjNdGb2JPBtoKuZrSRcpXQrMMnMLgCqgWHRy6cCZwBVwKfAzwDcfZOZ3QTMi153o7vvuRkuIpIoP/0pjB+fHm/aBJ0L6LpMc2+xrYJYlJaWekVFRdxliEiRefddOPro9PjBB+HCC+Orp6nMbL67N3idlY7lEBFpAnc4/XSYPj2MO3QIm9Xt28dbV7bkwa0cIiLJkLoBLhUQU6bAxx8XbkCAOgkRkQbt3AnHHBOe+QDhctbFi8MZTIVOnYSIyD48/3w4fC8VELNmhRvjiiEgQJ2EiMheff459OwJH30Uxt/+NrzySv7dMd1c6iRERPYwfjwcdFA6IBYsgL/8pfgCAtRJiIj83ZYt0KlTenz++eEE12KmTkJEBLj99voBUVWlgAB1EiJS5NauDXsPKVdfDXfcEV89SaOQEJGi9d//DX/8Y3q8Zg0cdlh89SSRlptEpOj87W9hEzoVELfdFu6kVkB8mToJESkqP/oRPPFEevzRR/UfDiT1qZMQkaLw9tuhe0gFxCOPhO5BAbFv6iREpKC5wymnwKuvhnHnzrB6NbRrF29d+UKdhIgUrFmzwoF8qYAoKwvPe1BANJ46CREpODt3Qv/+sGxZGB91VFhuKpbzllqSOgkRKShTpoQD+VIB8dprsGiRAmJ/6T+biBSEzz6Dbt3gk0/C+LvfhfLy4jxvqSWpkxCRvDduXHjwTyogFi6EadMUEC1BnYSI5K3Nm6FLl/R4+HB49NHYyilI6iREJC/dckv9gFi+XAGRDeokRCSvrF4NvXqlx9dcA7feGl89hU4hISJ548or4d570+N166B79/jqKQZabhKRxFu2LGxCpwLizjvDndQKiOxTJyEiieUO//mf8PTT6bktW+CQQ+KrqdiokxCRRJo/PxypkQqIxx8PoaGAyC11EiKSKLt3w7e+BW+8Ecbdu8OKFdC2bbx1FSt1EiKSGDNnQqtW6YB48cWwOa2AiI86CRGJ3Y4dcOSR8OGHYTxgAFRUhMCQeKmTEJFYTZ4MbdqkA+KNN2DBAgVEUqiTEJFYfPJJeADQjh1hfOaZ8PzzOm8padRJiEjOjRkDHTumA2LxYnjhBQVEEqmTEJGc2bgRunZNjy+8EB58ML56pGHqJEQkJ268sX5AVFcrIPKBOgkRyaqVK6F37/T4N7+Bm26Krx5pGoWEiGTNJZeE/YeU2tr63YQkX9aXm8zsQzN718zeNrOKaK6LmU03s2XR+87RvJnZvWZWZWYLzey4bNcnIi2vsjJsQqcCYtSocKSGAiL/5GpP4hR3H+DupdF4JDDT3fsBM6MxwBCgX/Q2Ahjzpe8kIonlDmedBf37h7EZbNsGl10Wb12y/+LauB4KjI8+Hg+cVWf+MQ/mAJ3MrGccBYpI08ydGw7ke+65MH7yyXAOU8eO8dYlzZOLPQkHppmZA39y97FAD3dfE31+LdAj+rgXUFPna1dGc2vqzGFmIwidBkcccUQWSxeRhuzeDSeeCPPmhXGvXuFRom3axFuXtIxcdBInu/txhKWkS83sX+t+0t2dECSN5u5j3b3U3Uu7devWgqWKSFNMmxaOz0gFRHl5uJpJAVE4st5JuPuq6P16M5sCnACsM7Oe7r4mWk5aH718FVDnYjlKojkRSZDt26Fv3/C8aYB//meYMycsN0lhyer/UjPrYGYHpz4GBgGLgDJgePSy4UC0ikkZ8JPoKqcTgS11lqVEJAEmTgxHd6cC4s030/sRUniy3Un0AKZYOJClNfCEu79sZvOASWZ2AVANDItePxU4A6gCPgV+luX6RKSRPv4YDj44PT77bHjmGZ23VOiyGhLuvhw4Zi/zG4FT9zLvwKXZrElEmu6+++Dyy9Pjykr4+tfjq0dyR3dci0hGGzZA3WtD/uu/4P7746tHck+riCKyV9dfXz8gamoUEMVIISEi9VRXh32G1CF8N9wQ7qQuKYm3LomHlptE5O8uvBAeeig93rgRunSJrx6JnzoJEWHx4tA9pAJizJjQPSggRJ2ESBFzD8+WfumlMG7bNnQPHTrEW5ckhzoJkSI1e3a4AS4VEE8/DZ9/roCQ+tRJiBSZXbvg+OPhnXfCuG9fWLoUDjww3rokmdRJiBSRqVOhdet0QMyYEU5sVUBIJuokRIrAF1+ES1g3bAjjk06C117TeUvSMP2IiBS4xx+Hdu3SAVFRAa+/roCQxlEnIVKgtm6FQw9Nj4cNCye46kA+aQr9LiFSgO66q35AvP8+PPWUAkKaTp2ESAFZvx569EiPr7gC7rknvnok/6mTECkQI0fWD4hVqxQQ0nwKCZE898EHYRnpD38I45tvDndSH354vHVJYdByk0geGz4cHnssPd68GTp1iq8eKTzqJETy0MKFoXtIBcS4caF7UEBIS1MnIZJH3GHQoHCnNEDHjmGz+qCD4q1LCpc6CZE8kboBLhUQU6bAtm0KCMkudRIiCbdzJxx9NFRWhvGRR4bnP7TW317JAXUSIglWVhYO30sFxKxZ4cRWBYTkin7URBLos8+gZ0/YsiWMTzkFZs7UHdOSe+okRBLmkUegfft0QCxYAK+8ooCQeKiTEEmILVvqX8J6/vkwYUJ89YiAOgmRRLjttvoBUVWlgJBkUCchEqO1a8PeQ8rVV8Mdd8RXj8ieFBIiMbn6arjzzvR4zRo47LD46hHZGy03ieRYVVXYhE4FxG23hTupFRCSROokRHLo/PPhySfT448+qv9wIJGkUSchkgMLFoTuIRUQjzwSugcFhCSdOgmRLHIPN8K9+moYd+4Mq1dDu3bx1iXSWOokRLJk1qxwIF8qIMrKYNMmBYTkF3USIi1sxw7o3z9sUAMcdRS8/bbOW5L8pE5CpAVNmQJt2qQD4q9/hUWLFBCSv/SjK9ICPv0UunUL7yE8GOjll3XekuQ/dRIizTRuHHTokA6IhQuhvFwBIYUhcSFhZoPNbKmZVZnZyLjrEclk8+YQBBddFMbDh4ermb75zXjrEmlJiQoJM2sFjAaGAP2B88ysf7xViXzZzTdDly7p8fLl8OijsZUjkjWJCgngBKDK3Ze7+3ZgIjA05ppE/m7VqtA9XHddGI8cGbqHvn3jrUskW5IWEr2AmjrjldFcPWY2wswqzKyitrY2Z8VJcbviCigpSY/XrYNbbomvHpFcSFpINIq7j3X3Uncv7datW9zlSIF7//3QPYwaFcZ33RW6h+7d461LJBeSdgnsKqB3nXFJNCeSc+4wbBhMnpye27oVDj44vppEci1pncQ8oJ+Z9TWzNsC5QFnMNUkRqqgIR2qkAuLxx0NoKCCk2CSqk3D3nWZ2GVAOtAIedvfFMZclRWT3bjj5ZJg9O4y7d4cVK6Bt23jrEolLokICwN2nAlPjrkOKz8yZcNpp6fHUqTBkSHz1iCRB4kJCJNd27IB+/aC6OoyPPRbmzYNWreKtSyQJkrYnIZJTTz8dDuRLBcTs2fDWWwoIkRR1ElKUPvkkPABox44wPvNMeP55nbcksid1ElJ0xoyBjh3TAbF4MbzwggJCZG/USUjR2LgRunZNjy+6CMaOja8ekXygTkKKwg031A+I6moFhEhjqJOQglZTA0cckR5ff30IDBFpHIWEFKxLLgn7Dym1tfW7CRFpmJabpOBUVoZN6FRAjBoVjtRQQIg0nToJKRjucNZZUBad9mUWDuTr2DHeukTymToJKQhz54YD+VIBMXFiOIdJASHSPOokJK/t2gUDB8L8+WFcUgJ/+1u4i1pEmk+dhOSt8nJo3TodENOmhauZFBAiLUedhOSd7duhTx9YsyaMBw6EN94Iy00i0rL010ryysSJ4dkOqYB4802YM0cBIZIt6iQkL2zbBocckh6ffTY884zOWxLJNv3+JYk3alT9gKishGefVUCI5II6CUms2trw+NCUSy6B0aPjq0ekGKmTkET6zW/qB0RNjQJCJA4KCUmU6uqwjPT734fxjTeGO6lLSuKtS6RYablJEuPCC+Ghh9LjjRuhS5f46hERdRKSAIsXh+4hFRAPPBC6BwWESPzUSUhs3OGMM+Dll8O4bdvQPXToEG9dIpKmTkJikbpDOhUQkyfD558rIESSRp2E5NSuXXDccbBwYRj/wz/Ae+/BgQfGW5eI7J06CcmZqVPDgXypgJg5M5zYqoAQSS51EpJ1X3wRLmHdsCGMTz4ZXn1V5y2J5AP9NZWsevxxaNcuHRAVFfDXvyogRPKFOgnJiq1b4dBD0+Nhw8IJrjpvSSS/6Pc5aXF33lk/IN5/H556SgEhko/USUiLWbcODjssPb7ySrj77vjqEZHmUychLeKaa+oHxOrVCgiRQqBOQpplwwbo1i09vvlmuPba+OoRkZalkJD9NmVKeMZDyubN0KlTfPWISMvTcpM02dq1cM458IMfhCWmiopwDpMCQqTwKCSk0dzhscegf38oKwvPfJg7F44/Pu7KRCRbshYSZvb/zGyVmb0dvZ1R53PXmlmVmS01s9PrzA+O5qrMbGS2apOmq66GIUNg+HD4p3+Cd96BX/9aR2qIFLps70nc5e531J0ws/7AucBRwOHADDM7Mvr0aOC7wEpgnpmVufuSLNco+7B7N4wZAyNHhk5i1KiwD6E7pkWKQxwb10OBie7+BfCBmVUBJ0Sfq3L35QBmNjF6rUIiJkuXhqfFvf46DBoEf/oT9OkTd1UikkvZ/n3wMjNbaGYPm1nnaK4XUFPnNSujuUzzX2JmI8yswswqamtrs1F3UduxA265BY45Jjw17tFHw3MfFBAixadZIWFmM8xs0V7ehgJjgP8FDADWAH9sgXoBcPex7l7q7qXd6l6kL822YAEMHBj2G773PViyJOxD6EgNkeLUrOUmdz+tMa8zsweBF6LhKqB3nU+XRHPsY16y7PPP4cYb4bbboGvX8KS4H/4w7qpEJG7ZvLqpZ53h2cCi6OMy4Fwza2tmfYF+wFxgHtDPzPqaWRvC5nZZtuqTtP/5HxgwICwx/eQnoXtQQIgIZHfj+jYzGwA48CFwMYC7LzazSYQN6Z3Ape6+C8DMLgPKgVbAw+6+OIv1Fb1t28Ky0ujRcMQRUF4eNqhFRFLM3eOuoVlKS0u9oqIi7jLyTnk5jBgBNTVw+eXhxriOHeOuSkRyxczmu3tpQ6/T1e5FZtOmsBE9eDC0bx+eEnfPPQoIEdk7hUQRmTw53C09YQJcd124kumkk+KuSkSSTKfAFoE1a+Cyy+DZZ+HYY8NS04ABcVclIvlAnUQBc4dHHgkH8r34Itx6aziQTwEhIo2lTqJAffhh2JiePh2+9S0YNw6OPLLBLxMRqUedRIHZtQvuvRe+8Q2YPTtc3jprlgJCRPaPOokCUlkJF1wQwmHw4HAg3xFHxF2ViOQzdRIFYMeOcJ/DgAHh5NbHHoOpUxUQItJ86iTy3Pz58POfw8KFMGxYWGrq0SPuqkSkUKiTyFOffRYeBDRwINTWwpQp8NRTCggRaVnqJPLQa6+FhwEtWxb2IO64Azp1irsqESlE6iTyyNat4dGh//ZvsHMnzJgRLm1VQIhItigk8sTUqeGy1gcegF/+Et59F049Ne6qRKTQabkp4TZsgKuugj//Odw5/cYbcOKJcVclIsVCnURCucOkSSEYJk6E3/4W3npLASEiuaVOIoFWrw57D889B6WlYe/h6KPjrkpEipE6iQRxh4ceCt1DeTncfnu4e1oBISJxUSeREMuXw0UXwSuvhKuXxo2Dr30t7qpEpNipk4jZrl1w113hyqV588LVS6+8ooAQkWRQSMRo0qQQDr/6FXznO7BkCVx8MRyg/ysikhBaborBxx/DoYfC7t1hPGECnHcemMVbl4jInvQ7a47dfz8cfHA6IBYvhvPPV0CISDKpk8iRjRuha9f0eMSI8LwHEZEkUyeRA7/7Xf2AqK5WQIhIflAnkUU1NfUf/HP99XDDDfHVIyLSVAqJLLn4Yhg7Nj2ura3fTYiI5AMtN7WwysqwCZ0KiPvuC3dSKyBEJB+pk2gh7vD978MLL4TxAQfAli3QsWO8dYmINIc6iRYwZ04IhVRATJwY7qRWQIhIvlMn0Qy7dsEJJ4QjvAF694aqKmjTJt66RERaijqJ/fTyy9C6dTogpk2DFSsUECJSWNRJNNEXX8BXvwrr1oXxwIHhaXE6b0lECpH+aWuCJ56Adu3SATF3bno/QkSkEKmTaIRt2+CQQ9Ljs8+GZ57ReUsiUvj0O3AD7rmnfkC89x48+6wCQkSKgzqJDGproXv39PiSS2D06PjqERGJQ7M6CTM7x8wWm9luMyvd43PXmlmVmS01s9PrzA+O5qrMbGSd+b5m9mY0/5SZxXad0HXX1Q+ImhoFhIgUp+YuNy0CfgC8VnfSzPoD5wJHAYOB+82slZm1AkYDQ4D+wHnRawH+ANzl7l8DNgMXNLO2JquuDstIN98cxjfeGO6kLinJdSUiIsnQrJBw90p3X7qXTw0FJrr7F+7+AVAFnBC9Vbn7cnffDkwEhpqZAd8BJkdfPx44qzm1NdUFF0CfPunxxo3w29/msgIRkeTJ1sZ1L6CmznhlNJdp/ivAR+6+c4/5vTKzEWZWYWYVtbW1zSp00aLQPTz8cBg/8EDoHrp0ada3FREpCA1uXJvZDOCwvXzqOnd/ruVLapi7jwXGApSWlvr+fQ8YMgTKy8O4XbvQPbRv32JliojkvQZDwt1P24/vuwroXWdcEs2RYX4j0MnMWkfdRN3XZ8XRR4cuAmDyZPjhD7P5p4mI5KdsLTeVAeeaWVsz6wv0A+YC84B+0ZVMbQib22Xu7sBfgP+Ivn44kNUuZeRIGDQItm9XQIiIZNLcS2DPNrOVwL8AL5pZOYC7LwYmAUuAl4FL3X1X1CVcBpQDlcCk6LUA1wC/MrMqwh7FQ82prSE/+lFYajrwwGz+KSIi+c3CL/H5q7S01CsqKuIuQ0Qkr5jZfHcvbeh1OpZDREQyUkiIiEhGCgkREclIISEiIhkpJEREJCOFhIiIZKSQEBGRjPL+PgkzqwWq9/PLuwIbWrCcXFDNuZOPdedjzZCfded7zV91924NfUHeh0RzmFlFY24mSRLVnDv5WHc+1gz5WXex1KzlJhERyUghISIiGRV7SIyNu4D9oJpzJx/rzseaIT/rLoqai3pPQkRE9q3YOwkREdkHhYSIiGSkkADM7GozczPrGnctjWFmt5vZe2a20MymmFmnuGvKxMwGm9lSM6sys5Fx19MQM+ttZn8xsyVmttjMroy7pqYws1ZmtsDMXoi7lsYws05mNjn6ea40s3+Ju6bGMLOrop+PRWb2pJm1i7umPZnZw2a23swW1ZnrYmbTzWxZ9L5zQ9+n6EPCzHoDg4AVcdfSBNOBb7j70cD7wLUx17NXZtYKGA0MAfoD55lZ/3iratBO4Gp37w+cCFyaBzXXdSXhqY/54h7gZXf/OnAMeVC7mfUCrgBK3f0bQCvCo5iT5lFg8B5zI4GZ7t4PmBmN96noQwK4C/i/QN7s4Lv7tOhRsABzgJI469mHE4Aqd1/u7tuBicDQmGvaJ3df4+5vRR9vI/yj1SveqhrHzEqAM4FxcdfSGGZ2KPCvRI8qdvft7v5RvFU1WmvgIDNrDbQHVsdcz5e4+2vApj2mhwLjo4/HA2c19H2KOiTMbCiwyt3fibuWZvg58FLcRWTQC6ipM15JnvyDC2BmfYBjgTfjraTR7ib8wrM77kIaqS9QCzwSLZGNM7MOcRfVEHdfBdxBWH1YA2xx92nxVtVoPdx9TfTxWqBHQ19Q8CFhZjOidcM934YCvwauj7vGvWmg7tRrriMsj0yIr9LCZGYdgWeAX7r71rjraYiZfQ9Y7+7z466lCVoDxwFj3P1Y4BMasfwRt2gdfygh5A4HOpjZj+Otquk83P/Q4ApK6xzUEit3P21v82b2TcL/5HfMDMKSzVtmdoK7r81hiXuVqe4UM/sp8D3gVE/uzS6rgN51xiXRXKKZ2YGEgJjg7s/GXU8jnQR838zOANoBh5jZn909yf94rQRWunuqU5tMHoQEcBrwgbvXApjZs8D/Bv4ca1WNs87Merr7GjPrCaxv6AsKvpPIxN3fdffu7t7H3fsQfmCPS0JANMTMBhOWFb7v7p/GXc8+zAP6mVlfM2tD2Nwri7mmfbLwG8NDQKW73xl3PY3l7te6e0n0s3wu8ErCA4Lo71qNmf1jNHUqsCTGkhprBXCimbWPfl5OJQ823CNlwPDo4+HAcw19QcF3EgXqPqAtMD3qgua4+y/iLenL3H2nmV0GlBOuAHnY3RfHXFZDTgL+D/Cumb0dzf3a3afGWFMhuxyYEP0SsRz4Wcz1NMjd3zSzycBbhOXeBSTwiA4zexL4NtDVzFYCvwNuBSaZ2QWERywMa/D7JHelQkRE4la0y00iItIwhYSIiGSkkBARkYwUEiIikpFCQkREMlJIiIhIRgoJERHJ6P8DxmXsInB3rgkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kG8hSp7WHCD"
      },
      "source": [
        "### f. Reflection (5 points)\n",
        "Recall that in class we noted that OLS and FLD can be equivalent under certain situations. Is that the case here? If not, briefly comment on why they performed differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa2X0WzPCFKw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hte8xdEpZNZ"
      },
      "source": [
        "## **Question 4:** An algorithm using SGD (33 points)\n",
        "Let us consider a different linear classifier in this problem, one based on the idea of an $\\textit{error}$ function minimization problem, similar to what we studied with linear regression, but whose error function is more tailored to the classification problem.\n",
        "\n",
        "The form of our linear classifier is $h_{\\bf{w}}(\\bf{x}) = \\mbox{sgn}(\\bf{w}^T \\bf{x})$, where $sgn$ is the sign function (so $1$ if $\\bf{w}^T \\bf{x} \\geq 0$ and $-1$ otherwise).  We are assuming here that we have padded the inputs with an extra dimension of $1$ to account for the bias term in the linear function, as we did with linear regression.\n",
        "\n",
        "In the binary classification case, each target class $y_i$ is either $1$ or $-1$.  One possible error function to use would say that the error is 0 if our linear classifier agrees with the target label (i.e. $y_i = \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), and if our linear classifier predicts the incorrect class ($y_i \\neq \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), then the error will be given by $|\\bf{w}^T \\bf{x}_i|$---intuitively, we penalize more for predictions that are farther from being predicted correctly.\n",
        "\n",
        "**Note:** the analytical parts of this question assume column feature vectors. However, you should still assume row feature vectors in the coding parts for consistency with our datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19cL4fQjsHHC"
      },
      "source": [
        "### Problem a. (5 points)\n",
        "Show that, for each training point, this error function may be concisely written as\n",
        "\n",
        "$L_i(\\bf{w}) = \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJyt3ZrBuG2"
      },
      "source": [
        "When the product wTxi has the same sign as -yi, the maximum of the loss is equal to 0. When wTxi has opposite sign as -yi, the loss is equal to yi wT xi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlqWT0CXs5vk"
      },
      "source": [
        "### Problem b. (5 points)\n",
        "We will compute the total loss over the training data as the sum of the $L_i$ losses, namely\n",
        "\n",
        "$L(\\bf{w}) = \\sum_{i=1}^n \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$.\n",
        "\n",
        "Note that one reason we chose this particular loss function is that it is continuous (unlike the 0-1 loss).  Furthermore, it has a simple sub-gradient.  The sub-gradient of the loss at 0 is 0, and everywhere else it is equal to the gradient.  Show that the sub-gradient is equal to\n",
        "$\\nabla_{\\bf{w}} L_i = \n",
        "\\begin{cases}\n",
        "0 & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ correctly}\\\\\n",
        "-y_i \\bf{x}_i & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ incorrectly.}\n",
        "\\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQ26ILVB8xY"
      },
      "source": [
        "When the loss ends up resulting in yi wT xi, the subgradient -yi xi displays how the value is predicted incorrectly. When the loss results in 0, the subgradient is also equal to 0, the correct prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF24kNfQuSKB"
      },
      "source": [
        "### Problem c. (5 points)\n",
        "Using a batch size of 1 and a step size of 1, write down the update rule for stochastic (sub)-gradient descent to minimize $L(\\bf{w})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BZZldGACCr_"
      },
      "source": [
        "w(t+1) = w(t) - 1 * gradient(w/respect to w) of Loss[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOT7Z6BrV6T"
      },
      "source": [
        "### Problem d. (12 points)\n",
        "Now it's time to implement the actual algorithm using the update rule we derived from part c. \n",
        "\n",
        "1.   Complete the one_pass function, which iterates over the entire Xext and y once, updating w in the process. (6 points)\n",
        "2.   Go over the training set twice (2 epochs) to get your final w, which you can do by calling one_pass twice in the get_wSGD function. Then report the CCR on the test set. (6 points)\n",
        "\n",
        "Note:\n",
        "* You should always shuffle the training set before each pass.\n",
        "* The first shuffle has already been performed by train_test_split, so you do not need to do a shuffle before the first pass here.\n",
        "* You can get rid of the one_pass function if you think it's better to incorporate it into get_wSGD, but you must keep get_wSGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14KZEtjvOuO",
        "outputId": "54995d9a-634c-420a-86be-90424377f54e"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def one_pass(Xext, y, w):\n",
        "    n = Xext.shape[0]\n",
        "    for i in range(0,n):\n",
        "        # WRITE CODE HERE:\n",
        "        if np.sign(w[i]*Xext[[i],1]) is not np.sign(y[i]):\n",
        "          w[i+1] = w[i] + y[i]*Xext[[i],1]\n",
        "        else:\n",
        "          w[i+1] = w[i]\n",
        "\n",
        "def get_wSGD(Xext, y):\n",
        "    # use 0 as the random state for shuffling\n",
        "    w = np.zeros(np.size(Xext))\n",
        "    np.random.seed(seed=0)\n",
        "    np.random.shuffle(Xext)\n",
        "    one_pass(Xext, y , w)\n",
        "    #w = np.ones(np.size(Xext))\n",
        "    np.random.seed(seed=0)\n",
        "    np.random.shuffle(Xext)\n",
        "    one_pass(Xext, y , w)\n",
        "    # WRITE CODE HERE:\n",
        "    return w\n",
        "\n",
        "wSGD = get_wSGD(Xtr_ext, ytrain)\n",
        "CCR = compute_CCR(Xte_ext, wSGD, ytest)\n",
        "print(\"The test CCR using SGD is\", CCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using SGD is 0.896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAyX-08Z1f5h"
      },
      "source": [
        "### Problem e. (6 points)\n",
        "\n",
        "The result you get from the previous part is likely not ideal. And if you change the random_state used for shuffling in the previous part (remember to change it back to 0 if you try this), you should see large swings in CCR. This is a common phenomenon when using SGD naively, and we will see this again in question 5.\n",
        "\n",
        "Recall from part c that we are using a fixed step size of 1, which is way too large for you to converge to a local minimum consistently. \n",
        "\n",
        "In practice, a technique called learning rate decay is commonly used with SGD. Essentially, the step size gets smaller after each iteration. This allows you to \"learn fast\" in the beginning and also be able to converge to a local minimum later when you are close to one.\n",
        "\n",
        "We will use an initial learning rate (step size) of 1 and the inverse square root decay, which means the step size at iteration $t$ is $\\frac1{\\sqrt t}$.\n",
        "\n",
        "Note:\n",
        "* $t$ starts at 1, not 0; otherwise, you will get a division by 0\n",
        "* You can get rid of the one_pass2 function if you think it's better to incorporate it into get_wSGD2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlM2SdxOvGvi",
        "outputId": "b5ea8a8a-5a3f-4351-a581-e257f1025c1d"
      },
      "source": [
        "\n",
        "def one_pass2(Xext, y, w, t):\n",
        "    n = Xext.shape[0]\n",
        "    for i in range(1,n):\n",
        "        # WRITE CODE HERE:\n",
        "        if np.sign(w[i]*Xext[i,1]) is not np.sign(y[i]):\n",
        "          w[i+1] = w[i] + t**(-1/2) * y[i] * Xext[i,1]\n",
        "          #t += 1\n",
        "        else:\n",
        "          w[i+1] = w[i]\n",
        "          #t += 1\n",
        "    \n",
        "\n",
        "def get_wSGD2(Xext, y):\n",
        "    # use 0 as the random state for shuffling\n",
        "    w = np.zeros(np.size(Xext))\n",
        "    t = 1\n",
        "    np.random.seed(seed=0)\n",
        "    np.random.shuffle(Xext)\n",
        "    one_pass2(Xext, y , w, t)\n",
        "    #w = np.ones(np.size(Xext))\n",
        "    np.random.seed(seed=0)\n",
        "    np.random.shuffle(Xext)\n",
        "    one_pass2(Xext, y , w, t)\n",
        "    # WRITE CODE HERE:\n",
        "    return w\n",
        "\n",
        "wSGD2 = get_wSGD2(Xtr_ext, ytrain)\n",
        "CCR = compute_CCR(Xte_ext, wSGD2, ytest)\n",
        "print(\"The test CCR using SGD with decay is\", CCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using SGD with decay is 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcnj3HYJHTqa"
      },
      "source": [
        "## **Question 5:** Testing the above methods on a real-world dataset (10 points)\n",
        "No code is required for this part. Getting part a to run correctly is not required for part b.\n",
        "\n",
        "The purpose of this question is to compare the three (four if you count SGD with decay as a separate one) methods on a real-world dataset. We will use the famous Iris dataset created by Sir Ronald Fisher (the same Fisher as in Fisher's Linear Discriminant). It contains feature measurements of 150 samples from three *Iris* flower species. We will combine two of the classes into a single negative class and the remaining class is the positive class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekpBYEcfB8-3"
      },
      "source": [
        "### Problem a. (6 points)\n",
        "Run the cells below. The code for this part is provided to you, but it requires several earlier functions to be properly implemented to run. They need to be able to handle data with multiple features.\n",
        "\n",
        "You will be graded on how well the behavior of this part matches our expectation. Do NOT modify the code provided here; if you have issues running it, check the code you wrote in previous parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usHZVolZXXen"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "\n",
        "# Split data into feature vectors and labels\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# change the label values for binary classification\n",
        "y[:100] = -1\n",
        "y[100:] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yfPgky0OvO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896d6d7f-9e24-4823-c62c-685a9fc40db2"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.25)\n",
        "Xtr_ext = np.c_[X_train, np.ones(X_train.shape[0])]\n",
        "Xte_ext = np.c_[X_test, np.ones(X_test.shape[0])]\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEUTOZkLTZxk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "6408b203-3c5f-4b4d-d5a7-57e10e6d4ca1"
      },
      "source": [
        "wOLS = get_wOLS_ext(X_train, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wOLS, y_test)\n",
        "print(\"The test CCR using OLS is\", CCR)\n",
        "\n",
        "X1, X2 = seperate(X_train, y_train)\n",
        "m1, m2 = get_means(X1, X2)\n",
        "Sw = get_Sw(X1, X2, m1, m2)\n",
        "print(m1.ndim)\n",
        "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
        "\n",
        "CCR = compute_CCR(Xte_ext, wFLD_ext, y_test)\n",
        "print(\"The test CCR using FLD is\", CCR)\n",
        "\n",
        "wSGD = get_wSGD(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD, y_test)\n",
        "print(\"The test CCR using SGD is\", CCR)\n",
        "\n",
        "wSGD2 = get_wSGD2(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD2, y_test)\n",
        "print(\"The test CCR using SGD with decay is\", CCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5,)\n",
            "The test CCR using OLS is 0.8947368421052632\n",
            "2\n",
            "0\n",
            "DIM M1\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7974501ca6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwFLD_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wFLD_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mCCR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_CCR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwFLD_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The test CCR using FLD is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCCR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3e02618c8a88>\u001b[0m in \u001b[0;36mcompute_CCR\u001b[0;34m(Xext, wext, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_CCR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#s1 = np.size(Xext)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_binary_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#sign = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfrac_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3e02618c8a88>\u001b[0m in \u001b[0;36mlinear_binary_predict\u001b[0;34m(Xext, wext)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m    \u001b[0;31m#   Xext = Xext[:,:-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 32 is different from 5)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxpckb9mCE_P"
      },
      "source": [
        "### Problem b. (4 points)\n",
        "A class containing sub-classes is quite common in real-world classifications. One direct effect of this is that within such a class, the feature vectors tend to be quite spread out (in distinct clusters). Comment on the effect this may have on OLS and FLD.\n",
        "\n",
        "Hint: If the previous part runs correctly, you should see OLS and FLD perform worse than the SGD algorithm from problem 4e. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71oQW2IFCt7"
      },
      "source": [
        "Spread out feature vectors will make correlation more expensive to compute no matter the algorithm. SGD performs most accurately because its dimensionality reduction vectorizes loss in a way that converges to the minimum more quickly over time.  Therefore OLS and FLD will perform considerably worse with harder-to compute data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOi0LTgR_Yhz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}